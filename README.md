# LLM Prompt Runner & Logging Harness  
**Pet Project 1 â€“ Foundation for LLM Failure Analysis & Debugging Toolkit**

This repository contains **Project 1** of a multi-stage capstone focused on building an **LLM Failure Analysis & Debugging Toolkit**.

The purpose of this project is to create a **reliable prompt execution engine** and a **structured logging harness** that captures everything needed for downstream analysis (latency, metadata, outputs).

---

## ğŸ¯ Project Goal

Build a simple but robust system that:

- Runs prompts/tasks against Large Language Models (LLMs)
- Measures execution metadata (timestamps, latency, model info)
- Logs every run in a structured format (JSONL)
- Serves as the **data backbone** for later failure analysis

This project focuses on **observability and infrastructure**, not evaluation yet.

---

## ğŸ§  Why This Matters

To debug LLM behavior, you must first **observe it**.

This logging harness enables future capabilities such as:
- Failure detection
- Hallucination analysis
- Prompt debugging
- Model comparison
- Automated reporting

---

## ğŸ§© Project Components

### Prompt Runner (`run.py`)
- CLI-based prompt execution
- Sends user prompts to an LLM
- Prints model responses
- Measures:
  - request timestamp (UTC)
  - execution latency
- Passes all data to the logger

---

### LLM Client (`llm/client.py`)
- Handles communication with the LLM provider
- Uses environment variables for API keys
- Abstracted to support multiple providers later

---

### Logging Harness (`logger/run_logger.py`)
- Receives prompt, response, and metadata
- Writes **one JSON object per run**
- Uses JSON Lines (`.jsonl`) format
- Designed for append-only logging and easy ingestion

---

### Run Data (`data/runs.jsonl`)
- Stores execution logs
- One line = one prompt run
- Not committed to version control

---

## ğŸ“ Project Structure
llm-failure-toolkit/
â”‚
â”œâ”€â”€ run.py # Main prompt runner
â”œâ”€â”€ llm/
â”‚ â””â”€â”€ client.py # LLM client abstraction
â”‚
â”œâ”€â”€ logger/
â”‚ â””â”€â”€ run_logger.py # Logging harness
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ runs.jsonl # Runtime logs (gitignored)
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt



---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/<your-username>/llm-failure-toolkit.git
cd llm-failure-toolkit


2ï¸âƒ£ Create and activate a virtual environment
python -m venv llmenv


Windows
=================
llmenv\Scripts\activate


macOS / Linux
=======================
source llmenv/bin/activate

3ï¸âƒ£ Install dependencies
==============================
pip install -r requirements.txt

4ï¸âƒ£ Configure environment variables
===================================
Create a .env file (not committed) and add your API key:
=================================
GEMINI_API_KEY=your_api_key_here


5ï¸âƒ£ Run the prompt runner
========================
python run.py

---
## ---------------------------------------------------------------------------------------------- ##
** Pet Project-2 Validator of the model **
Verify what does the model, do different and check its failed cases, understand its failure points
Store the validation results, 
Update it into my jsons file

The whole objective is to validate the outputs given by the model

## Updated ğŸ“ Project Structure

llm-failure-toolkit/
â”‚
â”œâ”€â”€ run.py # Main prompt runner
â”œâ”€â”€ llm/
â”‚ â””â”€â”€ client.py # LLM client abstraction
|
|___ validators/
|   â””â”€â”€ __init__.py
|   â””â”€â”€ base.py
|   â””â”€â”€ basic_validators.py
|
â”œâ”€â”€ logger/
â”‚ â””â”€â”€ run_logger.py # Logging harness
â”‚
â”œâ”€â”€ data/
â”‚ â””â”€â”€ runs.jsonl # Runtime logs (gitignored)
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt